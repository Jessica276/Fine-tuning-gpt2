# -*- coding: utf-8 -*-
"""gpt2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ubfCzEsxQ1V7XSyYQ4atVJA9GBzLIN7P
"""

!pip install chardet

import gdown
import chardet
import pandas as pd
import csv
import sys
import re
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split

url = "https://drive.google.com/uc?id=1-QyZJXzDcwZ3ZqIBk33l6WYXb_OHM51S"
output = "rakitra.csv"
gdown.download(url,output)

#Detect the encoding
with open(output, "rb") as f:
  result = chardet.detect(f.read())

print(result)

csv.field_size_limit(sys.maxsize)

with open(output, "r", encoding="utf-8") as inf, open("test.csv", "w", encoding="utf-8", newline='') as outf:
    # newline='' to avoid extra blank rows in the output file
    reader = csv.reader(inf)
    writer = csv.writer(outf)

    for row in reader:
        if len(row) > 0:  # Ignore les lignes vides
            writer.writerow(row)

# data = pd.read_csv("test.csv", encoding=result["encoding"])
# data.head()

try:
    data = pd.read_csv("test.csv", encoding="utf-8")  # First try utf-8
except UnicodeDecodeError:
    data = pd.read_csv("test.csv", encoding="latin-1")  # Fallback to latin-1 if utf-8 fails

data.shape

"""# Preprocessing"""

text_train, text_val = train_test_split(data, test_size=0.2, random_state=42)

"""# Load model"""

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2")
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")

batch_size = 132
learning_rate = 1e-4
epochs = 20
max_seq_len = 50

class DataHandler(Dataset):
    def __init__(self, texts, max_seq_len, tokenizer):
        self.texts = texts
        self.max_seq_len = max_seq_len
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        texts = str(self.texts.iloc[idx])
        inputs = self.tokenizer(
            texts,
            max_length=self.max_seq_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )

        return inputs["input_ids"].squeeze()

dataset = DataHandler(text_train["text"], max_seq_len, tokenizer)

data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# def train_model(model, epochs):
#   model = model.to(device)
#   optimizer = optim.Adam(model.parameters(), lr=learning_rate)
#   scheluder = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)

#   for epoch in range(epochs):
#     model.train()
#     total_loss = 0

#     for batch in data_loader:
#       inputs = batch.to(device)
#       labels = inputs.clone()

#       optimizer.zero_grad()
#       outputs = model(inputs, labels=labels)
#       loss = outputs.loss
#       total_loss += loss.item()

#       loss.backward()
#       optimizer.step()

#     scheluder.step()
#     avg_loss = total_loss / len(data_loader)

#     print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}")

#   return model

def train_model(model, epochs):
  model = model.to(device)
  optimizer = optim.Adam(model.parameters(), lr=learning_rate)
  scheluder = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)

  # Gradient accumulation steps
  accumulation_steps = 4  # Adjust as needed

  for epoch in range(epochs):
    model.train()
    total_loss = 0
    optimizer.zero_grad() # Zero the gradients before each epoch

    for i, batch in enumerate(data_loader):
      inputs = batch.to(device)
      labels = inputs.clone()

      outputs = model(inputs, labels=labels)
      loss = outputs.loss / accumulation_steps # Normalize the loss
      total_loss += loss.item()

      loss.backward()

      # Update model parameters every accumulation_steps
      if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad() # Reset gradients after each step

    scheluder.step()
    avg_loss = total_loss / len(data_loader)

    print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}")

  return model

model = train_model(model, epochs)



"""# Generate"""

text_val["text"][76]

#Generate text
input_text = "Efa hitanao miharihary"
input_ids = tokenizer.encode(input_text, return_tensors="pt")
output = model.generate(input_ids.to(device), max_length=100, num_return_sequences=1)

print(tokenizer.decode(output[0], skip_special_tokens=True))

data